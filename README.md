# Introduction

Sequence-to-Sequence (seq-2-seq) models for solving many-to-many problem tasks such as Machine Translation and Question Answering.

In this group project, you will implement a seq-2-seq model with and without attention mechanism for developing a generative chatbot using one of the datasets listed below.

Choose one of the datasets from the list provided below:

1. Question Answer
    1. The WikiQA Corpus was made publicly available in 2015, and has been updated several times since its inception. It contains different sets of question and sentence pairs that were originally collected.
    1. Question-Answer Dataset. This chatbot dataset was designed for use in Academic research, and features Wikipedia articles alongside manually-generated factoid questions that come from them. It also features manually-generated answers to the aforementioned questions.
1. Customer Support
    1. Ubuntu Dialogue Corpus: Consisting of almost one million two person conversations that have each been taken from the Ubuntu chat logs, this dataset is perfect for training a chatbot. It contains 930,000 dialogues spanning 100,000,000 words.

## Tasks

Carry out the following tasks:

1. Develop chatbot based on the following models using one of the chosen datasets.
    1. Seq-2-seq model without attention
    1. Seq-2-seq model with attention – You can use either Bahdanau or Luong Attention mechanism in the project
1. Carry out necessary pre-processing tasks to prepare the data
1. Split the dataset into appropriate Train/Validation/Test sets
1. Use the Validation set to train the model
1. Evaluate your model on the Test dataset
    1. You can assess the performance of the two models in terms of their accuracy or BLEU score
    1. You can also manually evaluate (on a smaller subset), answers generated by a chatbot
1. Record a 10-minute video presentation, intended for interested stakeholders, that summarizes the work carried out for tasks 1 – 5, and which presents salient chatbot development results obtained by your group.

## Deliverables

In addition to the video presentation, your group should produce a write-up (in MS-Word or pdf format) that addresses and reports on the work carried out for tasks 1-5. The project report will be expected to have about 1,500 words but note that it is the quality of the report that matters, not the number of words. The Python code must be included in your submission.

## Task division

1. Dataset selection (Ubuntu Dialogu Corpus)
1. Initiation
    1. Dataset analysis - format, quality
    1. Research - theory (Seq2Seq, attention - Bahdanau, Luong), how to measure accuracy (BLEU)
    1. High-level design / code structure / frameworks
    1. Agree toolset & frameworks
    1. Create plan
1. Implementation
    1. Data load
    1. Data cleansing
    1. Model training
    1. Model evaluation
1. Write-up
    1. Report
    1. Presentation
    1. Video

## Assumptions

* We do collaborative experimental / early development in a Jupyter Notebook in Google Collab
* All code will be clean, well structured and well commented
* Pending decision - will we submit Python code or a Jupyter Notebook?

## Solution Components

1. Data analysis ([Ubuntu Dialog Corpus](https://www.kaggle.com/datasets/rtatman/ubuntu-dialogue-corpus)) (Matt)
    1. The data contains many variations of Question-Answer
    1. Need NLP to recognise a question vs. an answer?
    1. Investigate with Pandas / Y Profiler (Nathan)
1. Data cleansing (Mahad & Zsuzsanna)
    1. Remove non-ASCII characters
    1. Save Question-Answer pairs only (use spaCy / NLTK?)
    1. Write to dataset_cleansed_«size».csv (or use to_pickle?)
1. Encoder-decoder LSTM implementation (PyTorch) (Teng & Nathan)
    1. Load the Pickle file
    1. Training
    1. Save / load trained model - Pickle
1. Incorporate attention (Teng & Nathan)
    1. Luong or Bahdanau
1. Model evaluation (Matt / All)
    1. Accuracy / BLEU score
    1. Manual testing - command line "chatbot"
    1. Hyper-parameter tuning & evaluation
    1. Graphs
1. ??? Train a open source chatbot on a small dataset and evaluate ??? (Matt - clarify what is meant by "You can also manually evaluate (on a smaller subset), answers generated by a chatbot")

### Ways of working

* Checkpoint this Friday (virtual), then Tuesday (Video).
* Ask Zsuzsanna to start the report - MS Word in SharePoint
* For each component - consider "design", references, ... as they are being developed - in MS Word

## Frameworks & Tooling

* [spaCy](https://spacy.io/api)
* [PyTorch](https://pytorch.org/text/stable/index.html)
* [pandas](https://pandas.pydata.org/docs/reference/index.html#api)
* [NumPy](https://numpy.org/doc/stable/reference/index.html#reference)

## References

* [Ubuntu Dialogue Corpus](https://arxiv.org/pdf/1506.08909)
* [PyTorch Seq2Seq Translation Tutorial](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)
* [PyTorch Seq2Seq Tutorial](https://github.com/bentrevett/pytorch-seq2seq)
* [Sequence to Sequence Learning with Neural Networks](https://github.com/bentrevett/pytorch-seq2seq/blob/main/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb)
